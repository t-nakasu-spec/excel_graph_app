# app.py
# --------------------------------------------
# Excelã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ æ¡ä»¶ã‚·ãƒ¼ãƒˆã®åˆ—ä½ç½®ã«åŸºã¥ãå›³ç•ªã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚° â†’ æ—¥/é€±/æœˆé›†è¨ˆ â†’ å¤šè»¸ã‚°ãƒ©ãƒ•ï¼ˆStreamlitï¼‰
# ä»•æ§˜è¦ç‚¹ï¼š
# - æ—¥ä»˜åˆ—ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€ŒçŠ¶æ…‹ã€
# - æ¡ä»¶ã‚·ãƒ¼ãƒˆï¼šBåˆ—=å‡ºè·å“ç•ªã€Cåˆ—ä»¥é™=ã™ã¹ã¦ã‚°ãƒ©ãƒ•ç•ªå·ï¼ˆåˆ—åã¯ä½•ã§ã‚‚OKï¼‰
# - å·¦è»¸ï¼ˆæ£’ï¼‰ï¼šç”Ÿç”£æ¸ˆãƒ»ç”Ÿç”£æ™‚é–“[åˆ†]ã€å³è»¸ï¼ˆç·šï¼‰ï¼šå·¥æ•°
# - ç•°å¸¸å€¤ãƒ•ã‚£ãƒ«ã‚¿ã€ç²’åº¦ï¼ˆæ—¥/é€±/æœˆï¼‰ã€æœŸé–“æŒ‡å®šã€CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# --------------------------------------------

import numpy as np
import pandas as pd
import streamlit as st
import altair as alt
import plotly.graph_objects as go
from plotly.subplots import make_subplots

st.set_page_config(page_title="Excelã‚°ãƒ©ãƒ•åŒ–ãƒ„ãƒ¼ãƒ«ï¼ˆæ¡ä»¶ã‚·ãƒ¼ãƒˆå¯¾å¿œï¼‰", layout="wide")
st.title("ğŸ“Š Excelã‚°ãƒ©ãƒ•åŒ–ãƒ„ãƒ¼ãƒ«ï¼ˆæ¡ä»¶ã‚·ãƒ¼ãƒˆå¯¾å¿œï¼‰")

st.markdown(
    "- **ç·é›†è¨ˆï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿åˆç®—ï¼‰** â†’ **å„ã‚°ãƒ©ãƒ•åï¼ˆæ¡ä»¶ã‚·ãƒ¼ãƒˆ Cåˆ—ä»¥é™ï¼‰ã”ã¨**ã«è©²å½“ã€Œå‡ºè·å“ç•ªã€ã‚’**åˆç®—ã—ã¦**è¡¨ç¤º  \n"
    "- å·¦è»¸ï¼ˆæ£’ï¼‰: **ç”Ÿç”£æ¸ˆ**ãƒ»**ç”Ÿç”£æ™‚é–“[åˆ†]** ï¼ å³è»¸ï¼ˆç·šï¼‰: **å·¥æ•°**  \n"
    "- æ¡ä»¶ã‚·ãƒ¼ãƒˆã¯ **Båˆ—=å‡ºè·å“ç•ª**ã€**Cåˆ—ä»¥é™=ã‚°ãƒ©ãƒ•ç•ªå·ï¼ˆåˆ—åã¯ä»»æ„ï¼‰** ã¨ã—ã¦è‡ªå‹•è§£é‡ˆã—ã¾ã™"
)

# -----------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -----------------------------
DATE_CANDIDATES = ["çŠ¶æ…‹", "ç”Ÿç”£æ—¥", "å‡ºè·æ—¥", "æ›´æ–°æ—¥æ™‚"]

def parse_datetime_series(s: pd.Series) -> pd.Series:
    """æ—¥ä»˜/æ—¥æ™‚ã£ã½ã„åˆ—ã‚’datetimeã¸ã€‚Excelã‚·ãƒªã‚¢ãƒ«/æ–‡å­—åˆ—/NaTã«å¯¾å¿œã€‚"""
    if s is None:
        return pd.Series([], dtype="datetime64[ns]")
    if np.issubdtype(s.dtype, np.datetime64):
        try:
            return s.dt.tz_localize(None)
        except Exception:
            return s
    # æ•°å€¤ï¼ˆExcelæ—¥æ•°ã‚·ãƒªã‚¢ãƒ«å¯¾å¿œï¼‰
    if np.issubdtype(s.dtype, np.number):
        try:
            return pd.to_datetime(s, unit="D", origin="1899-12-30", errors="coerce")
        except Exception:
            pass
    # æ–‡å­—åˆ—ãªã©
    return pd.to_datetime(s, errors="coerce", infer_datetime_format=True)

def ensure_numeric(s: pd.Series, fill=0) -> pd.Series:
    return pd.to_numeric(s, errors="coerce").fillna(fill)

def compute_minutes(soyo_time: pd.Series, mode: str) -> pd.Series:
    x = ensure_numeric(soyo_time, fill=0)
    return x * 1440.0 if mode == "excel_time" else x

def pick_default_date_col(df: pd.DataFrame) -> str:
    if "çŠ¶æ…‹" in df.columns:
        return "çŠ¶æ…‹"
    for c in DATE_CANDIDATES:
        if c in df.columns:
            return c
    for c in df.columns:
        if np.issubdtype(df[c].dtype, np.datetime64):
            return c
    return df.columns[0] if len(df.columns) else "æ—¥ä»˜"

def normalize_conditions_by_position(cond_raw: pd.DataFrame):
    """
    æ¡ä»¶ã‚·ãƒ¼ãƒˆã‚’åˆ—ä½ç½®ã§æ­£è¦åŒ–ï¼š
      - å‡ºè·å“ç•ª: ç‰©ç†Båˆ—ï¼ˆindex=1ï¼‰
      - ã‚°ãƒ©ãƒ•ç•ªå·: ç‰©ç†Cåˆ—ï¼ˆindex=2ï¼‰ä»¥é™ã‚’ã™ã¹ã¦å¯¾è±¡
    æˆ»ã‚Šå€¤: (æ­£è¦åŒ–DataFrame, ã‚°ãƒ©ãƒ•åˆ—åãƒªã‚¹ãƒˆ)
    """
    if cond_raw is None or cond_raw.empty:
        return pd.DataFrame(columns=["å‡ºè·å“ç•ª"]), []

    cond = cond_raw.copy()
    cols = list(cond.columns)

    # Båˆ— â†’ å‡ºè·å“ç•ª
    if len(cols) >= 2:
        cond.rename(columns={cols[1]: "å‡ºè·å“ç•ª"}, inplace=True)
        cond["å‡ºè·å“ç•ª"] = cond["å‡ºè·å“ç•ª"].astype(str).str.strip()
    else:
        cond["å‡ºè·å“ç•ª"] = np.nan

    # Cä»¥é™ â†’ ã™ã¹ã¦ã‚°ãƒ©ãƒ•åˆ—ã¨ã—ã¦æ‰±ã†ï¼ˆåˆ—åã¯ä½•ã§ã‚‚å¯ï¼‰
    # ç©ºåˆ—ãƒ•ã‚£ãƒ«ã‚¿ã‚’ä¸€åº¦ã ã‘å®Ÿè¡Œ
    graph_cols = []
    if len(cols) >= 3:
        graph_cols = [c for c in cols[2:] if cond[c].notna().any()]

    # æœ€ä½é™ã®åˆ—ã ã‘æ®‹ã™
    keep_show = ["å‡ºè·å“ç•ª"] + graph_cols
    cond = cond[[c for c in keep_show if c in cond.columns]].copy()

    # å‡ºè·å“ç•ª æ­£è¦åŒ–
    if "å‡ºè·å“ç•ª" in cond.columns:
        cond["å‡ºè·å“ç•ª"] = cond["å‡ºè·å“ç•ª"].astype(str).str.strip()

    return cond, graph_cols

def build_graph_map_dynamic(cond: pd.DataFrame, graph_cols: list[str]) -> dict:
    """
    ã‚°ãƒ©ãƒ•åï¼ˆã‚»ãƒ«å€¤ï¼‰â†’ {å‡ºè·å“ç•ª,...} ã®è¾æ›¸ã‚’ç”Ÿæˆã€‚
    graph_cols ã®å„åˆ—ã«æ›¸ã‹ã‚ŒãŸã‚»ãƒ«ã®å€¤ã‚’â€œã‚°ãƒ©ãƒ•åâ€ã¨ã—ã¦æ‰±ã†ã€‚
    """
    mapping: dict[str, set] = {}
    if "å‡ºè·å“ç•ª" not in cond.columns or not graph_cols:
        return mapping

    for _, row in cond.iterrows():
        item = str(row["å‡ºè·å“ç•ª"]).strip()
        if not item or item.lower() == "nan":
            continue
        for c in graph_cols:
            g = row.get(c, None)
            if pd.isna(g):
                continue
            gname = str(g).strip()
            if gname == "" or gname.lower() == "nan":
                continue
            mapping.setdefault(gname, set()).add(item)
    return mapping

def aggregate_timeseries(df: pd.DataFrame, date_col: str, freq: str) -> pd.DataFrame:
    """
    æ—¥ä»˜åˆ—ã§é›†è¨ˆï¼ˆfreq='D'|'W'|'M'ï¼‰ã€‚å·¥æ•°=ç”Ÿç”£æ™‚é–“[åˆ†]/ç”Ÿç”£æ¸ˆï¼ˆ0é™¤ç®—=0ï¼‰ã€‚
    """
    _df = df.copy()
    _df[date_col] = parse_datetime_series(_df[date_col])
    _df = _df.dropna(subset=[date_col])

    if _df.empty:
        return pd.DataFrame()

    _df["ç”Ÿç”£æ¸ˆ"] = ensure_numeric(_df.get("ç”Ÿç”£æ¸ˆ", pd.Series(dtype=float)), 0)
    _df["ç”Ÿç”£æ™‚é–“[åˆ†]"] = ensure_numeric(_df.get("ç”Ÿç”£æ™‚é–“[åˆ†]", pd.Series(dtype=float)), 0)
    _df["åŸºæº–æ™‚é–“[åˆ†]"] = ensure_numeric(_df.get("åŸºæº–æ™‚é–“[åˆ†]", pd.Series(dtype=float)), 0)
    _df["èƒ½ç‡[%]"] = ensure_numeric(_df.get("èƒ½ç‡[%]", pd.Series(dtype=float)), 0)

    _df = _df.set_index(date_col).sort_index()
    grouped = _df.resample(freq).agg({"ç”Ÿç”£æ¸ˆ": "sum", "ç”Ÿç”£æ™‚é–“[åˆ†]": "sum", "åŸºæº–æ™‚é–“[åˆ†]": "sum", "èƒ½ç‡[%]": "mean"})
    grouped["å·¥æ•°"] = np.where(grouped["ç”Ÿç”£æ¸ˆ"] > 0, grouped["ç”Ÿç”£æ™‚é–“[åˆ†]"] / grouped["ç”Ÿç”£æ¸ˆ"], 0.0)

    grouped = grouped.reset_index().rename(columns={date_col: "æ—¥ä»˜"})
    # æ—¥ä»˜åˆ—ã‚’ç¢ºå®Ÿã«datetimeå‹ã«ä¿æŒ
    if "æ—¥ä»˜" in grouped.columns:
        grouped["æ—¥ä»˜"] = pd.to_datetime(grouped["æ—¥ä»˜"], errors="coerce")
    return grouped

def build_summary_stats(agg_df: pd.DataFrame, columns_list: list = None) -> dict:
    """
    é›†è¨ˆçµæœã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡º
    å…¥åŠ›ï¼š
      - agg_df: é›†è¨ˆæ¸ˆã¿DataFrame
      - columns_list: ['å·¥æ•°', 'èƒ½ç‡[%]']ãªã©å¯¾è±¡åˆ—ã®ãƒªã‚¹ãƒˆ
    å‡ºåŠ›ï¼š
      {
        'å·¥æ•°': {'åˆè¨ˆ': 100.5, 'å¹³å‡': 10.05, 'æœ€å¤§': 25.3, 'æœ€å°': 2.1},
        'èƒ½ç‡[%]': {'åˆè¨ˆ': 920.0, 'å¹³å‡': 92.0, 'æœ€å¤§': 98.5, 'æœ€å°': 85.0}
      }
    """
    if columns_list is None:
        columns_list = ['å·¥æ•°', 'èƒ½ç‡[%]']
    
    summary = {}
    for col in columns_list:
        if col in agg_df.columns:
            valid_data = agg_df[col].dropna()
            if len(valid_data) > 0:
                summary[col] = {
                    'åˆè¨ˆ': valid_data.sum(),
                    'å¹³å‡': valid_data.mean(),
                    'æœ€å¤§': valid_data.max(),
                    'æœ€å°': valid_data.min()
                }
    return summary

def display_summary_metrics(agg_df: pd.DataFrame, columns_list: list = None):
    """
    çµ±è¨ˆæƒ…å ±ã‚’Streamlit metricsã§è¡¨ç¤º
    å…¥åŠ›ï¼š
      - agg_df: é›†è¨ˆæ¸ˆã¿DataFrame
      - columns_list: ['å·¥æ•°', 'èƒ½ç‡[%]']ãªã©å¯¾è±¡åˆ—ã®ãƒªã‚¹ãƒˆ
    å‡¦ç†ï¼š
      - DataFrameç©ºãƒã‚§ãƒƒã‚¯ â†’ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºã§ return
      - st.columns(2) ã§å·¦å³2åˆ—ã‚’ä½œæˆ
      - å„åˆ—ã«å·¥æ•° / èƒ½ç‡[%] ã‚’è¡¨ç¤º
      - å„æŒ‡æ¨™ï¼ˆåˆè¨ˆãƒ»å¹³å‡ãƒ»æœ€å¤§ãƒ»æœ€å°ï¼‰ã‚’ st.metric() ã§ç©ã¿é‡ã­
    """
    if columns_list is None:
        columns_list = ['å·¥æ•°', 'èƒ½ç‡[%]']
    
    if agg_df.empty or len(agg_df) == 0:
        st.info("é›†è¨ˆçµæœãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    stats = build_summary_stats(agg_df, columns_list)
    
    if not stats:
        st.info("é›†è¨ˆçµæœãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # 2åˆ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    cols = st.columns(len(columns_list))
    
    for idx, col_name in enumerate(columns_list):
        if col_name in stats:
            with cols[idx]:
                st.metric(f"{col_name} - åˆè¨ˆ", f"{stats[col_name]['åˆè¨ˆ']:.1f}")
                st.metric(f"{col_name} - å¹³å‡", f"{stats[col_name]['å¹³å‡']:.1f}")
                st.metric(f"{col_name} - æœ€å¤§", f"{stats[col_name]['æœ€å¤§']:.1f}")
                st.metric(f"{col_name} - æœ€å°", f"{stats[col_name]['æœ€å°']:.1f}")

def alt_dual_axis_chart(agg_df: pd.DataFrame, title: str, show_items: dict = None, y_autorange: bool = False):
    """
    Plotlyã‚’ä½¿ã£ãŸå¤šè»¸ã‚°ãƒ©ãƒ•
    å·¦è»¸ï¼šæ£’ï¼ˆç”Ÿç”£æ¸ˆãƒ»ç”Ÿç”£æ™‚é–“[åˆ†]ãƒ»åŸºæº–æ™‚é–“[åˆ†]ï¼‰/ å³è»¸1ï¼šå·¥æ•° / å³è»¸2ï¼šèƒ½ç‡[%]
    show_items: è¡¨ç¤ºè¦ç´ ã®è¾æ›¸
    y_autorange: Trueã§ Yè»¸ã‚ºãƒ¼ãƒ è¨±å¯ã€Falseã§å›ºå®š
    """
    if show_items is None:
        show_items = {"ç”Ÿç”£æ¸ˆ": True, "ç”Ÿç”£æ™‚é–“[åˆ†]": True, "åŸºæº–æ™‚é–“[åˆ†]": True, "å·¥æ•°": True, "èƒ½ç‡[%]": True}
    
    if agg_df.empty:
        return go.Figure().add_annotation(text="ãƒ‡ãƒ¼ã‚¿ãªã—", showarrow=False)
    
    _df = agg_df.copy()
    if "æ—¥ä»˜" in _df.columns:
        _df["æ—¥ä»˜"] = pd.to_datetime(_df["æ—¥ä»˜"], errors='coerce')
    _df = _df.replace([np.inf, -np.inf], np.nan)

    # Plotlyå›³ã‚’ä½œæˆï¼ˆ3ã¤ã®Yè»¸ï¼šå·¦ã€å³1ã€å³2ï¼‰
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    
    # å·¦è»¸ï¼šæ£’ã‚°ãƒ©ãƒ•ï¼ˆç”Ÿç”£æ¸ˆã€ç”Ÿç”£æ™‚é–“[åˆ†]ã€åŸºæº–æ™‚é–“[åˆ†]ï¼‰
    bar_configs = [
        ("ç”Ÿç”£æ¸ˆ", '#4472C4', 0.7),
        ("ç”Ÿç”£æ™‚é–“[åˆ†]", '#70AD47', 0.6),
        ("åŸºæº–æ™‚é–“[åˆ†]", '#FFC000', 0.6)
    ]
    
    for item_name, color, opacity in bar_configs:
        if show_items.get(item_name, True) and item_name in _df.columns:
            fig.add_trace(
                go.Bar(
                    x=_df["æ—¥ä»˜"],
                    y=_df[item_name],
                    name=item_name,
                    marker_color=color,
                    opacity=opacity,
                    yaxis='y'
                ),
                secondary_y=False
            )
    
    # å³è»¸ï¼šå·¥æ•°ãƒ©ã‚¤ãƒ³
    if show_items.get("å·¥æ•°", True) and "å·¥æ•°" in _df.columns:
        fig.add_trace(
            go.Scatter(
                x=_df["æ—¥ä»˜"],
                y=_df["å·¥æ•°"],
                name="å·¥æ•°",
                mode='lines+markers',
                line=dict(color='#F39C12', width=3),
                yaxis='y2'
            ),
            secondary_y=True
        )
    
    # å³è»¸2ï¼šèƒ½ç‡[%]ãƒ©ã‚¤ãƒ³ï¼ˆåˆ¥ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    if show_items.get("èƒ½ç‡[%]", True) and "èƒ½ç‡[%]" in _df.columns:
        fig.add_trace(
            go.Scatter(
                x=_df["æ—¥ä»˜"],
                y=_df["èƒ½ç‡[%]"],
                name="èƒ½ç‡[%]",
                mode='lines+markers',
                line=dict(color='#E74C3C', width=3, dash='dash'),
                yaxis='y3'
            )
        )

    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
    fig.update_layout(
        title=title,
        xaxis=dict(title="æ—¥ä»˜", domain=[0, 0.88], tickformat="%mæœˆ%dæ—¥<br>%Yå¹´"),
        yaxis=dict(title="ç”Ÿç”£æ¸ˆãƒ»æ™‚é–“[åˆ†]", side='left', fixedrange=not y_autorange),
        yaxis2=dict(title="å·¥æ•°", side='right', overlaying='y', title_font=dict(color='#F39C12'), tickfont=dict(color='#F39C12'), fixedrange=not y_autorange),
        yaxis3=dict(title="èƒ½ç‡[%]", side='right', overlaying='y', anchor='free', position=1.0, title_font=dict(color='#E74C3C'), tickfont=dict(color='#E74C3C'), fixedrange=not y_autorange),
        margin=dict(r=150),
        hovermode='x unified',
        height=500,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )
    
    return fig

# -----------------------------
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šã‚ªãƒ—ã‚·ãƒ§ãƒ³
# -----------------------------
with st.sidebar:
    st.header("âš™ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³")

    uploaded = st.file_uploader("Excelãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.xlsxï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])

    st.markdown("**ç•°å¸¸å€¤ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ'ç•°å¸¸å€¤'åˆ—ï¼‰**")
    abnormal_filter = st.radio(
        "ãƒ•ã‚£ãƒ«ã‚¿",
        options=[("å…¨ã¦", "all"), ("æ­£å¸¸ã®ã¿ï¼ˆ0ï¼‰", "normal"), ("ç•°å¸¸ã®ã¿ï¼ˆ1ï¼‰", "abnormal")],
        format_func=lambda x: x[0],
        horizontal=True,
        index=0,
    )[1]

    freq_options = [("æ—¥æ¬¡", "D"), ("é€±æ¬¡", "W"), ("æœˆæ¬¡", "M")]
    freq_choice = st.selectbox("é›†è¨ˆç²’åº¦", options=freq_options, format_func=lambda x: x[0], index=0)
    freq = freq_choice[1]  # ã‚¿ãƒ—ãƒ«ã®2ç•ªç›®ã®è¦ç´ ï¼ˆæ–‡å­—åˆ—ï¼‰ã‚’å–å¾—

    st.divider()
    st.markdown("**ã‚°ãƒ©ãƒ•è¡¨ç¤ºè¦ç´ **")
    show_seisansu = st.checkbox("ç”Ÿç”£æ¸ˆ", value=True)
    show_seisan_time = st.checkbox("ç”Ÿç”£æ™‚é–“[åˆ†]", value=True)
    show_kijun_time = st.checkbox("åŸºæº–æ™‚é–“[åˆ†]", value=True)
    show_kosuu = st.checkbox("å·¥æ•°", value=True)
    show_nouritsu = st.checkbox("èƒ½ç‡[%]", value=True)

    st.divider()
    st.markdown("**ã‚°ãƒ©ãƒ•æ“ä½œ**")
    y_autorange_mode = st.checkbox("Yè»¸è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ«", value=False)

    st.divider()
    st.caption("â€» ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆ0å§‹ã¾ã‚Šï¼‰ã‚’èª¿æ•´ã§ãã¾ã™ã€‚æœ€ä¸Šæ®µãŒè¦‹å‡ºã—ã§ãªã„å ´åˆã«ã”åˆ©ç”¨ãã ã•ã„ã€‚")
    cond_header_idx = st.number_input("æ¡ä»¶ã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ", min_value=0, max_value=50, value=0, step=1)
    data_header_idx = st.number_input("ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆï¼ˆ39ï¼‰ã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ", min_value=0, max_value=50, value=0, step=1)

if not uploaded:
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ Excel ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.xlsxï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# -----------------------------
# Excelèª­è¾¼
# -----------------------------
with st.spinner("Excelã‚’èª­ã¿è¾¼ã¿ä¸­â€¦"):
    try:
        xl = pd.ExcelFile(uploaded, engine="openpyxl")
    except Exception as e:
        st.error(f"Excelã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.stop()

    # æ¡ä»¶ã‚·ãƒ¼ãƒˆåã®æ¨å®š
    cond_sheet_name = "æ¡ä»¶ã‚·ãƒ¼ãƒˆ" if "æ¡ä»¶ã‚·ãƒ¼ãƒˆ" in xl.sheet_names else next((s for s in xl.sheet_names if "æ¡ä»¶" in s), None)
    if not cond_sheet_name:
        st.error(f"æ¡ä»¶ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å­˜åœ¨ã™ã‚‹ã‚·ãƒ¼ãƒˆ: {xl.sheet_names}")
        st.stop()

    # ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆå
    data_sheet_name = "39" if "39" in xl.sheet_names else xl.sheet_names[0]

    try:
        cond_raw = xl.parse(cond_sheet_name, header=int(cond_header_idx))
        data_raw = xl.parse(data_sheet_name, header=int(data_header_idx))
    except Exception as e:
        st.error(f"ã‚·ãƒ¼ãƒˆã®èª­ã¿å–ã‚Šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.stop()

# æ¡ä»¶ã‚·ãƒ¼ãƒˆæ­£è¦åŒ–ï¼ˆåˆ—ä½ç½®ãƒ™ãƒ¼ã‚¹ï¼‰
cond, graph_cols = normalize_conditions_by_position(cond_raw)
gmap = build_graph_map_dynamic(cond, graph_cols)
graph_names = sorted(gmap.keys())

# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
data = data_raw.copy()

# æ—¥ä»˜åˆ—ã®æ—¢å®šã¯ã€ŒçŠ¶æ…‹ã€
date_col_default = pick_default_date_col(data)

# æ—¥ä»˜åˆ—å€™è£œã‚’æ§‹ç¯‰ï¼šDATE_CANDIDATESå„ªå…ˆã€ãã®å¾Œä¸­èº«ãŒæ—¥ä»˜ã®åˆ—ã‚’è£œå®Œ
date_options = [c for c in DATE_CANDIDATES if c in data.columns]
for c in data.columns:
    if c not in date_options and parse_datetime_series(data[c]).notna().any():
        date_options.append(c)
if not date_options and len(data.columns) > 0:
    date_options = [data.columns[0]]

if date_col_default not in date_options and date_col_default in data.columns:
    date_options.append(date_col_default)
    
date_col = st.selectbox(
    "æ—¥ä»˜åˆ—ã‚’é¸æŠï¼ˆæ—¢å®š=çŠ¶æ…‹ï¼‰",
    options=date_options or list(data.columns),
    index=(date_options or list(data.columns)).index(date_col_default) if (date_options or list(data.columns)) else 0
)

# æ•°å€¤åŒ–
data["ç”Ÿç”£æ¸ˆ"] = ensure_numeric(data.get("ç”Ÿç”£æ¸ˆ", pd.Series(dtype=float)), 0)
data["ç”Ÿç”£æ™‚é–“[åˆ†]"] = compute_minutes(data.get("æ‰€è¦æ™‚é–“", pd.Series(dtype=float)), "excel_time")

# åŸºæº–æ™‚é–“[åˆ†]ã®è¨ˆç®—ï¼ˆExcelå½¢å¼ Ã— 86400 / 60ï¼‰
if "åŸºæº–æ™‚é–“" in data.columns:
    data["åŸºæº–æ™‚é–“[åˆ†]"] = ensure_numeric(data.get("åŸºæº–æ™‚é–“", pd.Series(dtype=float)), 0) * 86400 / 60
else:
    data["åŸºæº–æ™‚é–“[åˆ†]"] = 0.0

# èƒ½ç‡[%]ã®è¨ˆç®—ï¼ˆåŸºæº–æ™‚é–“[åˆ†] / ç”Ÿç”£æ™‚é–“[åˆ†] Ã— 100ï¼‰
data["èƒ½ç‡[%]"] = np.where(
    data["ç”Ÿç”£æ™‚é–“[åˆ†]"] > 0,
    (data["åŸºæº–æ™‚é–“[åˆ†]"] / data["ç”Ÿç”£æ™‚é–“[åˆ†]"]) * 100,
    0.0
)

# ç•°å¸¸å€¤ãƒ•ã‚£ãƒ«ã‚¿
if "ç•°å¸¸å€¤" in data.columns:
    if abnormal_filter == "normal":
        data = data[data["ç•°å¸¸å€¤"].fillna(0) == 0]
    elif abnormal_filter == "abnormal":
        data = data[data["ç•°å¸¸å€¤"].fillna(0) == 1]
else:
    st.warning("æ³¨æ„ï¼š'ç•°å¸¸å€¤' åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ç•°å¸¸å€¤ãƒ•ã‚£ãƒ«ã‚¿ã¯ç„¡åŠ¹ã§ã™ã€‚")

# æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿UI
dt_series = parse_datetime_series(data.get(date_col))
if dt_series.notna().any():
    min_d, max_d = dt_series.min().date(), dt_series.max().date()
    c1, c2 = st.columns(2)
    with c1:
        start_date = st.date_input("é–‹å§‹æ—¥", value=min_d, min_value=min_d, max_value=max_d)
    with c2:
        end_date = st.date_input("çµ‚äº†æ—¥", value=max_d, min_value=min_d, max_value=max_d)
    mask = (dt_series.dt.date >= start_date) & (dt_series.dt.date <= end_date)
    data = data.loc[mask].copy()
else:
    st.warning("é¸æŠã—ãŸæ—¥ä»˜åˆ—ã‚’æ—¥æ™‚ã«è§£é‡ˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥ä»˜åˆ—ã®é¸æŠã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")

# ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
with st.expander("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­50è¡Œï¼‰", expanded=False):
    st.caption(f"ã‚·ãƒ¼ãƒˆ: {data_sheet_name} / è¡Œæ•°: {len(data)}")
    st.dataframe(data.head(50), use_container_width=True)

with st.expander("æ¡ä»¶ã‚·ãƒ¼ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­50è¡Œï¼‰", expanded=False):
    st.caption(f"ã‚·ãƒ¼ãƒˆ: {cond_sheet_name} / è¡Œæ•°: {len(cond)} / ã‚°ãƒ©ãƒ•åˆ—æ•°: {len(graph_cols)}")
    st.dataframe(cond.head(50), use_container_width=True)
st.divider()
# ---- ç·é›†è¨ˆï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿åˆç®—ï¼‰ ----
st.subheader("â‘  ç·é›†è¨ˆï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿åˆç®—ï¼‰")

st.caption(f"é›†è¨ˆå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data)} ä»¶")

# å“ç•ªé¸æŠUIï¼ˆç·é›†è¨ˆç”¨ï¼‰
all_hinban = sorted(data["å‡ºè·å“ç•ª"].astype(str).str.strip().unique()) if "å‡ºè·å“ç•ª" in data.columns else []
with st.expander("ğŸ”§ è¡¨ç¤ºæ¡ä»¶ï¼ˆå“ç•ªï¼‰", expanded=False):
    if all_hinban:
        selected_hinban_overall = st.multiselect(
            "è¡¨ç¤ºã™ã‚‹å“ç•ªã‚’é¸æŠ",
            options=all_hinban,
            default=all_hinban,
            key="overall_hinban_select"
        )
    else:
        selected_hinban_overall = []

# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
if selected_hinban_overall:
    data_filtered_overall = data[data["å‡ºè·å“ç•ª"].astype(str).str.strip().isin(selected_hinban_overall)].copy()
else:
    data_filtered_overall = data.copy()

# CSVç”¨ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã§é›†è¨ˆ
overall_agg = aggregate_timeseries(data, date_col=date_col, freq=freq)
# ã‚°ãƒ©ãƒ•ç”¨ã¯é¸æŠã•ã‚ŒãŸå“ç•ªã®ã¿ã§é›†è¨ˆ
overall_agg_filtered = aggregate_timeseries(data_filtered_overall, date_col=date_col, freq=freq)
st.caption(f"é›†è¨ˆçµæœ: {len(overall_agg)} è¡Œ")

# ãƒ‡ãƒãƒƒã‚°ï¼šé›†è¨ˆå¾Œã®ã‚«ãƒ©ãƒ ã‚’è¡¨ç¤º
with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°ï¼šé›†è¨ˆå¾Œã®ã‚«ãƒ©ãƒ ä¸€è¦§", expanded=False):
    st.write("**å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ :**")
    st.write(overall_agg.columns.tolist())
    st.write("**ãƒ‡ãƒ¼ã‚¿å‹:**")
    st.write(overall_agg.dtypes)
    st.write("**èƒ½ç‡[%]ã®å€¤ï¼ˆå…ˆé ­10è¡Œï¼‰:**")
    st.write(overall_agg[["æ—¥ä»˜", "èƒ½ç‡[%]"]].head(10))

if not selected_hinban_overall:
    st.warning("âš ï¸ å“ç•ªã‚’é¸æŠã—ã¦ãã ã•ã„")
elif overall_agg_filtered.empty:
    st.warning("âš ï¸ é›†è¨ˆçµæœãŒç©ºã§ã™ã€‚æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ã‚„æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.dataframe(data_filtered_overall[[date_col, "ç”Ÿç”£æ¸ˆ", "ç”Ÿç”£æ™‚é–“[åˆ†]"]].head(10))
else:
    st.dataframe(overall_agg_filtered.head(10))
    display_summary_metrics(overall_agg_filtered, ['å·¥æ•°', 'èƒ½ç‡[%]'])
    st.plotly_chart(alt_dual_axis_chart(overall_agg_filtered, "ç·é›†è¨ˆ", show_items={
    "ç”Ÿç”£æ¸ˆ": show_seisansu,
    "ç”Ÿç”£æ™‚é–“[åˆ†]": show_seisan_time,
    "åŸºæº–æ™‚é–“[åˆ†]": show_kijun_time,
    "å·¥æ•°": show_kosuu,
    "èƒ½ç‡[%]": show_nouritsu
}, y_autorange=y_autorange_mode), use_container_width=True, config={"scrollZoom": True})
st.download_button(
    "ç·é›†è¨ˆCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=overall_agg.to_csv(index=False).encode("utf-8-sig"),
    file_name="overall_aggregate.csv",
    mime="text/csv"
)

st.divider()

# ---- å„ã‚°ãƒ©ãƒ•åã”ã¨ ----
st.subheader("â‘¡ å„ã‚°ãƒ©ãƒ•åï¼ˆæ¡ä»¶ã‚·ãƒ¼ãƒˆ Cåˆ—ä»¥é™ï¼‰ã”ã¨ã®é›†è¨ˆ")
if not graph_names:
    st.info("æ¡ä»¶ã‚·ãƒ¼ãƒˆã«ã‚°ãƒ©ãƒ•åï¼ˆCåˆ—ä»¥é™ã®ã‚»ãƒ«å€¤ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
else:
    if "å‡ºè·å“ç•ª" not in data.columns:
        st.error("ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆã« 'å‡ºè·å“ç•ª' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ—åã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
    else:
        # ã‚°ãƒ©ãƒ•åé¸æŠ
        selected_gname = st.selectbox("è¡¨ç¤ºã™ã‚‹ã‚°ãƒ©ãƒ•ã‚’é¸æŠ", options=graph_names, key="graph_select")
        
        st.subheader(f"ğŸ“Š {selected_gname}")
        
        items = sorted(gmap[selected_gname])

        
        # è¡¨ç¤ºæ¡ä»¶ã‚’ expander ã§æŠ˜ã‚ŠãŸãŸã¿
        with st.expander("ğŸ”§ è¡¨ç¤ºæ¡ä»¶ï¼ˆå“ç•ªãƒ»æ—¥ä»˜ãƒ»é›†è¨ˆï¼‰", expanded=False):
            selected_items = st.multiselect(
                "è¡¨ç¤ºã™ã‚‹å“ç•ªã‚’é¸æŠ",
                options=items,
                default=items,
                key=f"hinban_select_{selected_gname}"
            )
            
            st.caption(f"å¯¾è±¡ å‡ºè·å“ç•ªï¼ˆ{len(items)}ä»¶ï¼‰ï¼š{', '.join(items[:30])}{' ...' if len(items) > 30 else ''}")

        # CSVç”¨ï¼šæ¡ä»¶ã‚·ãƒ¼ãƒˆè¨­å®šé€šã‚Šã®å…¨å“ç•ªãƒ‡ãƒ¼ã‚¿
        sub_all = data[data["å‡ºè·å“ç•ª"].astype(str).str.strip().isin(items)].copy()
        if sub_all.empty:
            st.warning(f"âš ï¸  '{selected_gname}': è©²å½“å‡ºè·å“ç•ªãƒ‡ãƒ¼ã‚¿ãªã—")
        elif not selected_items:
            st.warning(f"âš ï¸  '{selected_gname}': å“ç•ªã‚’é¸æŠã—ã¦ãã ã•ã„")
        else:
            # ã‚°ãƒ©ãƒ•ç”¨ï¼šé¸æŠã•ã‚ŒãŸå“ç•ªã®ã¿
            sub = data[data["å‡ºè·å“ç•ª"].astype(str).str.strip().isin(selected_items)].copy()
            if sub.empty:
                st.warning(f"âš ï¸  '{selected_gname}': é¸æŠã—ãŸå“ç•ªã®ãƒ‡ãƒ¼ã‚¿ãªã—")
            else:
                # CSVç”¨é›†è¨ˆï¼ˆå…¨å“ç•ªï¼‰
                agg_all = aggregate_timeseries(sub_all, date_col=date_col, freq=freq)
                # ã‚°ãƒ©ãƒ•ç”¨é›†è¨ˆï¼ˆé¸æŠå“ç•ªï¼‰
                agg = aggregate_timeseries(sub, date_col=date_col, freq=freq)
                
                # é›†è¨ˆçµæœãŒç©ºã®å ´åˆã®ãƒã‚§ãƒƒã‚¯
                if agg.empty:
                    st.error(f"âŒ '{selected_gname}': é›†è¨ˆçµæœãŒç©ºã§ã™ï¼ˆæ—¥ä»˜ãƒ»æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")
                else:
                    display_summary_metrics(agg, ['å·¥æ•°', 'èƒ½ç‡[%]'])
                    st.plotly_chart(alt_dual_axis_chart(agg, f"{selected_gname}", show_items={
                        "ç”Ÿç”£æ¸ˆ": show_seisansu,
                        "ç”Ÿç”£æ™‚é–“[åˆ†]": show_seisan_time,
                        "åŸºæº–æ™‚é–“[åˆ†]": show_kijun_time,
                        "å·¥æ•°": show_kosuu,
                        "èƒ½ç‡[%]": show_nouritsu
                    }, y_autorange=y_autorange_mode), use_container_width=True, config={"scrollZoom": True})
                    st.download_button(
                        f"{selected_gname} ã®é›†è¨ˆCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå…¨å“ç•ªï¼‰",
                        data=agg_all.to_csv(index=False).encode("utf-8-sig"),
                        file_name=f"aggregate_{selected_gname}.csv",
                        mime="text/csv"
                    )